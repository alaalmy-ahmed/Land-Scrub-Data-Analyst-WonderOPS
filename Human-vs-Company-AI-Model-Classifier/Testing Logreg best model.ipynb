{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Classified\n",
      "Data Filtered and Saved\n",
      "Summary report saved to 'Scrubbed/SL Property Export NM+RANCHOS+DE+TAOS.xlsx Statistical Report.txt'\n",
      "Files have been zipped to: Scrubbed/SL Property Export NM+RANCHOS+DE+TAOS.xlsx ZIPPED.zip\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import re\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='sklearn')\n",
    "\n",
    "EXCEL_FILE_PATH = \"SL Property Export NM+RANCHOS+DE+TAOS.xlsx\"\n",
    "MODEL_FILE_PATH = \"logreg_classifier.pickle\"\n",
    "VECTORIZER_FILE_PATH = \"logreg_vectorizer.pickle\"\n",
    "CLASSIFIED_FILE_PATH = f\"Classified/{EXCEL_FILE_PATH} Classified.xlsx\"\n",
    "FILTERED_FILE_PATH = f\"Scrubbed/{EXCEL_FILE_PATH} SCRUBBED.xlsx\"\n",
    "ZIP_FILE_PATH = f\"Scrubbed/{EXCEL_FILE_PATH} ZIPPED.zip\"\n",
    "summary_file_path = f\"Scrubbed/{EXCEL_FILE_PATH} Report.txt\"\n",
    "\n",
    "def load_data(file_path):\n",
    "    try:\n",
    "        return pd.read_excel(file_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {file_path}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def clean_data(df):\n",
    "    df.drop_duplicates(subset=['MAIL_ADDR', 'MAIL_CITY', 'MAIL_STATE', 'MAIL_ZIP'], inplace=True)\n",
    "    df.dropna(subset=['MAIL_ADDR', 'MAIL_CITY', 'MAIL_STATE', 'MAIL_ZIP', 'OWNER_NAME_1'], inplace=True)\n",
    "    df['Full Name'] = df['OWNER_NAME_1'].str.replace(',', '').str.replace('&', '')\n",
    "    return df\n",
    "\n",
    "def load_model_and_vectorizer(model_path, vectorizer_path):\n",
    "    try:\n",
    "        with open(model_path, \"rb\") as model_file:\n",
    "            model = pickle.load(model_file)\n",
    "        with open(vectorizer_path, \"rb\") as vec_file:\n",
    "            vectorizer = pickle.load(vec_file)\n",
    "        return model, vectorizer\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"File not found: {e.filename}\")\n",
    "        return None, None\n",
    "\n",
    "def predict_names(df, model, vectorizer):\n",
    "    if 'OWNER_NAME_1' in df.columns:\n",
    "        X_test = vectorizer.transform(df['Full Name'])\n",
    "        df['Prediction'] = model.predict(X_test)\n",
    "        return df\n",
    "    else:\n",
    "        print(\"The 'OWNER_NAME_1' column is missing from the Excel file.\")\n",
    "        return df\n",
    "\n",
    "def save_predictions(df, file_path):\n",
    "    df.to_excel(file_path, index=False)\n",
    "\n",
    "def filter_company_names(df, human_names):\n",
    "    df = df[df['OWNER_NAME_1'].isin(human_names)]\n",
    "    return df\n",
    "\n",
    "def remove_suspected_company_names(df, keywords):\n",
    "    pattern = r'\\b(?:' + '|'.join(map(re.escape, keywords)) + r')\\b'\n",
    "    df = df[~df['OWNER_NAME_1'].str.contains(pattern, case=False, na=False)]\n",
    "    return df\n",
    "\n",
    "def visualize_summary_statistics(df_classified):\n",
    "    stats = {}\n",
    "    df = load_data(f\"Data/{EXCEL_FILE_PATH}\")\n",
    "    # Count initial records\n",
    "    initial_count = len(df)\n",
    "    stats['Original Number of Records'] = initial_count\n",
    "\n",
    "    # Drop missing values and count remaining records\n",
    "    df_cleaned_missing = df.dropna(subset=['MAIL_ADDR', 'MAIL_CITY', 'MAIL_STATE', 'MAIL_ZIP', 'OWNER_NAME_1'])\n",
    "    missing_dropped_count = initial_count - len(df_cleaned_missing)\n",
    "    missing_dropped = (missing_dropped_count / initial_count)\n",
    "    stats['Missing Data Dropped (%)'] = missing_dropped\n",
    "    stats['Number of Missing Data'] = missing_dropped_count\n",
    "\n",
    "    # Drop duplicates and count remaining records (after dropping missing values)\n",
    "    df_cleaned_duplicates = df_cleaned_missing.drop_duplicates(subset=['MAIL_ADDR', 'MAIL_CITY', 'MAIL_STATE', 'MAIL_ZIP'])\n",
    "    duplicated_dropped_count = len(df_cleaned_missing) - len(df_cleaned_duplicates)\n",
    "    duplicated_dropped = (duplicated_dropped_count / len(df_cleaned_missing))\n",
    "    stats['Duplicated Data Dropped (%)'] = duplicated_dropped\n",
    "    stats['Number of Duplicated Data'] = duplicated_dropped_count\n",
    "\n",
    "    # Number of human names remaining\n",
    "    human_names_remaining = len(df_classified[df_classified['Prediction'] == 'Human Name'])\n",
    "    stats['Human Names Remaining'] = human_names_remaining\n",
    "\n",
    "    # Number of company names classified\n",
    "    company_names_classified = len(df_classified[df_classified['Prediction'] == 'Company Name'])\n",
    "    stats['Company Names Classified'] = company_names_classified\n",
    "\n",
    "    # Save summary report to a file\n",
    "    with open(summary_file_path, \"w\") as f:\n",
    "        f.write(\"---------------------------- Summary Report -----------------------------\\n\")\n",
    "\n",
    "        for metric, value in stats.items():\n",
    "            # Format metric name\n",
    "            metric_name = metric.replace('_', ' ').title()\n",
    "\n",
    "            # Format value based on type\n",
    "            if isinstance(value, float):\n",
    "                # Display float with one decimal place and '%' if it's between 0 and 1\n",
    "                if 0 <= value <= 1:\n",
    "                    formatted_value = f\"{value*100:.1f}%\"\n",
    "                else:\n",
    "                    formatted_value = f\"{value:,.0f}\"\n",
    "            else:\n",
    "                # Display integers with commas\n",
    "                formatted_value = f\"{value:,}\"\n",
    "\n",
    "            f.write(f\"{metric_name:<30} {formatted_value:>20}\\n\")\n",
    "\n",
    "        f.write(\"\\n-------------------------------------------------------------------------\\n\")\n",
    "\n",
    "    print(f\"Summary report saved to 'Scrubbed/{EXCEL_FILE_PATH} Statistical Report.txt'\")\n",
    "    return summary_file_path\n",
    "\n",
    "\n",
    "def zip_files(zip_file_path, files_to_zip):\n",
    "    with zipfile.ZipFile(zip_file_path, 'w') as zf:\n",
    "        for file in files_to_zip:\n",
    "            zf.write(file, os.path.basename(file))\n",
    "    print(f\"Files have been zipped to: {zip_file_path}\")\n",
    "\n",
    "\n",
    "# Main process\n",
    "df_initial = load_data(f\"Data/{EXCEL_FILE_PATH}\")\n",
    "df_cleaned = clean_data(df_initial)\n",
    "\n",
    "model, vectorizer = load_model_and_vectorizer(MODEL_FILE_PATH, VECTORIZER_FILE_PATH)\n",
    "if model and vectorizer:\n",
    "    df_classified = predict_names(df_cleaned, model, vectorizer)\n",
    "    save_predictions(df_classified[['OWNER_NAME_1', 'Prediction']], CLASSIFIED_FILE_PATH)\n",
    "    print(\"Data Classified\")\n",
    "     \n",
    "    filtered_df = load_data(CLASSIFIED_FILE_PATH)\n",
    "    human_names = filtered_df[filtered_df['Prediction'] == 'Human Name']['OWNER_NAME_1'].unique().tolist()\n",
    "    df_filtered = filter_company_names(df_cleaned, human_names)\n",
    "    save_predictions(df_filtered, FILTERED_FILE_PATH)\n",
    "    print(\"Data Filtered and Saved\")\n",
    "\n",
    "    # Generate summary statistics and save report\n",
    "    summary_file_path = visualize_summary_statistics(df_classified)\n",
    "\n",
    "    # Zip the filtered file and the summary report\n",
    "    zip_files(ZIP_FILE_PATH, [FILTERED_FILE_PATH, summary_file_path])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing data counter: 4, Missing data Percentage: 0.009009009009009009\n",
      "Missing data counter: 59, Missing data Percentage: 0.1340909090909091\n"
     ]
    }
   ],
   "source": [
    "df = load_data(f\"Data/{EXCEL_FILE_PATH}\")\n",
    "df_classsified = predict_names(df_cleaned, model, vectorizer)\n",
    "# Count initial records\n",
    "initial_count = len(df)\n",
    "df['Original Number of Records'] = initial_count\n",
    "\n",
    "# Drop missing values and count remaining records\n",
    "df_cleaned_missing = df.dropna(subset=['MAIL_ADDR', 'MAIL_CITY', 'MAIL_STATE', 'MAIL_ZIP', 'OWNER_NAME_1'])\n",
    "missing_dropped_count = initial_count - len(df_cleaned_missing)\n",
    "missing_dropped = (missing_dropped_count / initial_count)\n",
    "df['Missing Data Dropped (%)'] = missing_dropped\n",
    "df['Number of Missing Data'] = missing_dropped_count\n",
    "\n",
    "print(f\"Missing data counter: {missing_dropped_count}, Missing data Percentage: {missing_dropped}\")\n",
    "# Drop duplicates and count remaining records (after dropping missing values)\n",
    "df_cleaned_duplicates = df_cleaned_missing.drop_duplicates(subset=['MAIL_ADDR', 'MAIL_CITY', 'MAIL_STATE', 'MAIL_ZIP'])\n",
    "duplicated_dropped_count = len(df_cleaned_missing) - len(df_cleaned_duplicates)\n",
    "duplicated_dropped = (duplicated_dropped_count / len(df_cleaned_missing))\n",
    "df['Duplicated Data Dropped (%)'] = duplicated_dropped\n",
    "df['Number of Duplicated Data'] = duplicated_dropped_count\n",
    "print(f\"Missing data counter: {duplicated_dropped_count}, Missing data Percentage: {duplicated_dropped}\")\n",
    "\n",
    "# Number of human names remaining\n",
    "human_names_remaining = len(df_classified[df_classsified['Prediction'] == 'Human Name'])\n",
    "df['Human Names Remaining'] = human_names_remaining\n",
    "\n",
    "# Number of company names classified\n",
    "company_names_classified = len(df_classified[df_classified['Prediction'] == 'Company Name'])\n",
    "df['Company Names Classified'] = company_names_classified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
